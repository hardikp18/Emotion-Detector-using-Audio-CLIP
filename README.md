# Emotionâ€¯Detectorâ€¯withâ€¯AudioCLIP  
*by **Hardik Pahwa***

---

## âœ¨â€¯Motivation  
Most emotionâ€‘recognition systems handle **either** faces **or** voices.  
This project tries both at once. Using **AudioCLIP**â€”a version of OpenAIâ€™s  
**CLIP** that also understands soundâ€”the model learns to tell whether a person  
looks **and** sounds happy, sad, angry, etc.

---

## â€¯Where It Fits in Multimodal Learning  

| Year | Milestone | Why it matters |
|------|-----------|----------------|
| 2021 | **CLIP** â€“ links images and text | Largeâ€‘scale contrastive learning â†’ strong, reusable features |
| 2021 | **AudioCLIP** â€“ adds an audio branch to CLIP | One embedding space for **textâ€¯, â€¯imageâ€¯,â€¯audio** |
| 2023â€‘24 | Fineâ€‘tuning AudioCLIP for niche tasks | Works well with small datasets |
| **This repo** | Fineâ€‘tunes AudioCLIP on **FERâ€‘2013** (faces) + **CREMAâ€‘D** (speech) | Tests triâ€‘modal emotion recognition |

---

## ğŸ—’ï¸â€¯Project Overview  

* **Datasets**  
  * **FERâ€‘2013** â€“ 48Ã—48 grayscale face images, 7 emotions  
  * **CREMAâ€‘D** â€“ audio clips of actors speaking with 6 emotions  

* **Pipeline**  
  1. Convert WAV clips to melâ€‘spectrogram â€œimagesâ€.  
  2. Feed face images and spectrograms through the same vision backbone.  
  3. Use a contrastive loss so matching face/voice pairs stay close in the embedding space.  
  4. Add a small classifier head to predict the emotion label.

* **Training tricks** (see `jupyter notebook`)  
  * Mixedâ€‘precision (`torch.cuda.amp`)  
  * `AdamW` optimizer with cosine scheduler  
  * Early stopping on validation loss

---

## QuickÂ Start  

```bash
# 1. Clone
git clone https://github.com/yourâ€‘handle/emotionâ€‘detectorâ€‘audioclip.git
cd emotionâ€‘detectorâ€‘audioclip

# 2. (Optional) create virtual env
python -m venv venv
source venv/bin/activate                 # Windows: venv\Scripts\activate

# 3. Install packages
pip install -r requirements.txt

# 4. Put the datasets
#    data/fer2013/train/<emotion>/*.png
#    data/cremad/AudioWAV/*.wav

# 5. Train
python audio_clip.py                     # 20 epochs by default

# 6. Streamlit demo
streamlit run app.py

```

## What I Learned
Converting audio to 2â€‘D spectrograms lets one backbone handle both views.

A simple contrastive loss (image vs. audio) boosts performance by >â€¯3â€¯%.


## Reflections & Future Work
---
| â€ƒ                | Notes                                                                                             |
| ---------------- | ------------------------------------------------------------------------------------------------- |
| **Surprises**    | Fear and surprise look alike in lowâ€‘res faces; voice helps separate them.                         |
| **Limitations**  | Audio clips are cropped to 5â€¯s, missing long expressions.                                         |
| **Improvements** | Add video frames, use a larger ViT backbone, or train with dynamic time warping for longer audio. |
---

# References
A.Â Guzhov etâ€¯al., â€œAudioCLIP: Extending CLIP to Image, Text and Audio,â€ 2021.

GitHub: https://github.com/AndreyGuzhov/AudioCLIP

FERâ€‘2013 dataset (Kaggle).

CREMAâ€‘D dataset (Kaggle).

